{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd5b8fb",
   "metadata": {},
   "source": [
    "# Leitura da Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00ead3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import numpy\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb303b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Larissa\n",
      "[nltk_data]     Fossato\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b533baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('lista_papers'):\n",
    "    lista_papers = joblib.load('lista_papers')\n",
    "else:\n",
    "    list_of_files = os.listdir('scisummnet_release1.1__20190413/top1000_complete')\n",
    "\n",
    "    list_of_files2= []\n",
    "    for i in list_of_files:\n",
    "        nome = 'scisummnet_release1.1__20190413/top1000_complete/' + i + '/' + 'Documents_xml/' +os.listdir(f'scisummnet_release1.1__20190413/top1000_complete/{i}/Documents_xml')[0]\n",
    "        list_of_files2.append(nome)\n",
    "\n",
    "\n",
    "    lines = []\n",
    "    for file in list_of_files2:\n",
    "        f= open(file,'r')\n",
    "        lines.append(f.readlines())\n",
    "        f.close()\n",
    "\n",
    "    lista_papers=[]\n",
    "    for j in lines:\n",
    "        texto=''\n",
    "        for i in j:\n",
    "            texto = texto + str(i)\n",
    "        lista_papers.append(texto)\n",
    "\n",
    "        joblib.dump(lista_papers, 'lista_papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b546ef83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1009"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f264ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicando indices aos papers\n",
    "indices           = range(0,len(lista_papers))\n",
    "papers_com_indice = dict(zip(indices, lista_papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e3b9e5",
   "metadata": {},
   "source": [
    "# Estudo da Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b639643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af11bb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14eefd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_intro(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*[Ii][nN][tT][rR][oO][dD][Uu][Cc][Tt][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        #txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        lista.append(txt)\n",
    "    return txt\n",
    "\n",
    "def preprocessa_abstract(corpus: str):\n",
    "    regIter = re.finditer(\"(<[Aa][Bb][sS][tT][Rr][aA][cC][tT]>)(.+?)(</[aA][bB][sS][tT][rR][aA][cC][tT]>)\", corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        #txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        lista.append(txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def preprocessa_conclusion(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        #txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        lista.append(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "789abc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices_preprocessamento(x, funcao_preprocessamento):\n",
    "    \"\"\" alterar a funcao_preprocessamento entre os parâmetros:\n",
    "    preprocessa_intro, preprocessa_abstract, preprocessa_conclusion\"\"\"\n",
    "    papers_processados=[]\n",
    "    indices=[]\n",
    "    for i,j in zip(list(x.keys()),list(x.values())):\n",
    "        try:\n",
    "            papers_processados.append(funcao_preprocessamento(j))\n",
    "            indices.append(i)\n",
    "        except:\n",
    "            None\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bf3f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_papers_com_abstract   = get_indices_preprocessamento(papers_com_indice, preprocessa_abstract)\n",
    "indices_papers_com_introducao = get_indices_preprocessamento(papers_com_indice, preprocessa_intro)\n",
    "indices_papers_com_conclusao  = get_indices_preprocessamento(papers_com_indice, preprocessa_conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fa47f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtd de papers com abstract 924\n",
      "qtd de papers com introdução 802\n",
      "qtd de papers com conclusão 711\n",
      "---------------------------------\n",
      "qtd total de papers 1009\n"
     ]
    }
   ],
   "source": [
    "print('qtd de papers com abstract', len(indices_papers_com_abstract))\n",
    "print('qtd de papers com introdução', len(indices_papers_com_introducao))\n",
    "print('qtd de papers com conclusão', len(indices_papers_com_conclusao))\n",
    "\n",
    "print('---------------------------------')\n",
    "\n",
    "print('qtd total de papers', len(lista_papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa6b27",
   "metadata": {},
   "source": [
    "Contagem das sessões a partir do título faltante para melhoria do Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b090516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_sem_abstract = dict((key,value) for key, value in papers_com_indice.items() if key  not in  indices_papers_com_abstract)\n",
    "papers_sem_introducao = dict((key,value) for key, value in papers_com_indice.items() if key  not in  indices_papers_com_introducao)\n",
    "papers_sem_conclusao = dict((key,value) for key, value in papers_com_indice.items() if key  not in  indices_papers_com_conclusao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4aafd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contador_sessoes(x):\n",
    "    \"\"\"input: dicionário de papers\n",
    "       output: lista contendo o título das sessões\"\"\"\n",
    "    titulos_sessoes=[]\n",
    "    for i in x.values():\n",
    "        regIter = re.finditer('(<SECTION(.*?)>)', i, re.DOTALL|re.MULTILINE)\n",
    "        textos = [ t.group(2) for t in regIter]\n",
    "        titulos_sessoes.append(textos)\n",
    "\n",
    "    x=[]\n",
    "    for i in titulos_sessoes:\n",
    "        for j in i:\n",
    "            pattern = r'\"(.*?)\"'\n",
    "            matches = re.findall(pattern,str(j))\n",
    "            pattern2 = r'[0-9]'\n",
    "            new_string = re.sub(pattern2, '', matches[0])\n",
    "            x.append(new_string.strip())\n",
    "\n",
    "    frequency = collections.Counter(x)\n",
    "    return sorted(frequency.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65ef8445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 20),\n",
       " ('Introduction', 12),\n",
       " ('Acknowledgements', 6),\n",
       " ('Results', 5),\n",
       " ('', 4),\n",
       " ('Related Work', 4),\n",
       " ('Acknowledgments', 3),\n",
       " ('Abstract  Introduction', 3),\n",
       " ('Data and Evaluation', 3),\n",
       " ('Concluding Remarks', 3),\n",
       " ('Conclusion', 2),\n",
       " ('Experimental Results', 2),\n",
       " ('Conclusions and Future Work', 2),\n",
       " ('Conditional Random Fields', 2),\n",
       " ('The Joyce System in the Ulysses User Interface', 1),\n",
       " ('The Structure of Joyce', 1),\n",
       " ('The Text Planner', 1),\n",
       " ('The Sentence Planner', 1),\n",
       " ('The Linguistic Realizer', 1),\n",
       " ('An Example', 1),\n",
       " ('ENTER AGENT #&lt;information&gt; OBJECT INCOMPONENT Black Box&gt; LOCATION #&lt;PORT P&gt;',\n",
       "  1),\n",
       " ('Porting the System', 1),\n",
       " ('TRANSLATION DELIVERY SYSTEM', 1),\n",
       " ('TESTING AND EVALUATING MULTI-ENGINE PERFORMANCE', 1),\n",
       " ('CURRENT AND FUTURE WORK', 1),\n",
       " ('Input Structure', 1),\n",
       " ('System Architecture', 1),\n",
       " ('Linguistic Knowledge Bases', 1),\n",
       " ('Interfaces', 1),\n",
       " ('System Performance', 1),\n",
       " ('Status', 1),\n",
       " ('. Corpus-based Methods', 1),\n",
       " ('Arabic Language and Data', 1),\n",
       " ('SVM Based Approach', 1),\n",
       " ('Evaluation', 1),\n",
       " ('Conclusions &amp; Future Directions', 1),\n",
       " ('Previous Work', 1),\n",
       " ('Open IE in TEXTRUNNER', 1),\n",
       " ('Morpheme Segmentation', 1),\n",
       " ('Unsupervised Acquisition of New Stems', 1),\n",
       " ('Performance Evaluations', 1),\n",
       " ('Summary and Future Work', 1),\n",
       " ('Acknowledgment', 1),\n",
       " ('The Nature of Relations in English', 1),\n",
       " ('Relation Extraction', 1),\n",
       " ('Hybrid Relation Extraction', 1),\n",
       " ('Artificial Intelligence Center SRI International', 1),\n",
       " ('Traditional Solutions and an Alternative Approach', 1),\n",
       " ('Technical Preliminaries', 1),\n",
       " (\"Using Restriction to Extend Ear-. ley's Algorithm for PATR-II\", 1),\n",
       " ('Applications', 1),\n",
       " ('Encountering new words', 1),\n",
       " ('Bilingual lexicon as seed words', 1),\n",
       " ('Ranking translation candidates', 1),\n",
       " ('Confidence on seed word pairs', 1),\n",
       " ('Related work', 1),\n",
       " ('Discussions', 1),\n",
       " ('Conclusions', 1),\n",
       " ('Linear and Syntactic Order of a Sentence', 1),\n",
       " ('Projective Dependency Grammars Revisited', 1),\n",
       " ('A Formalization of', 1),\n",
       " ('A Polynomial Parser for PP-GDG', 1),\n",
       " ('Task description', 1),\n",
       " ('Chunk Types', 1),\n",
       " ('Support Vector Machines', 1),\n",
       " ('Approach for Chunk Identification', 1),\n",
       " ('Discussion', 1),\n",
       " ('Participating Systems', 1),\n",
       " ('Efficient Feature Induction for CRFs', 1),\n",
       " ('Web-augmented Lexicons', 1),\n",
       " ('Feature Set', 1),\n",
       " ('Results and Discussion', 1),\n",
       " ('METEOR: Metric for Evaluation of Translation with Explicit ORdering', 1),\n",
       " ('The METEOR Metric . Weaknesses in BLEU Addressed in METEOR', 1),\n",
       " ('Evaluation of the METEOR Metric', 1),\n",
       " ('Future Work', 1),\n",
       " ('Features', 1),\n",
       " ('Inductive Deterministic Parsing', 1),\n",
       " ('Non-Projective Relations', 1),\n",
       " ('Performance', 1),\n",
       " ('Experiments', 1),\n",
       " ('Error Analysis', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contador_sessoes(papers_sem_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30fbe38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 549),\n",
       " ('', 83),\n",
       " ('Acknowledgments', 63),\n",
       " ('Conclusion', 15),\n",
       " ('Motivation', 13),\n",
       " ('Acknowledgements', 9),\n",
       " ('Conclusions', 9),\n",
       " ('Related Work', 8),\n",
       " ('Definition', 8),\n",
       " ('Evaluation', 7),\n",
       " ('Background', 5),\n",
       " ('Acknowledgment', 4),\n",
       " ('Experiments', 4),\n",
       " ('Results', 4),\n",
       " ('Appendix', 3),\n",
       " ('References', 3),\n",
       " ('Conclusion and Future Work', 3),\n",
       " ('Discussion', 3),\n",
       " ('ACKNOWLEDGEMENTS', 3),\n",
       " ('Conclusions and future work', 2),\n",
       " ('Overview', 2),\n",
       " ('Related Work.', 2),\n",
       " ('. Results', 2),\n",
       " ('AT&amp;T Bell Laboratories AT&amp;T Bell Laboratories', 2),\n",
       " ('Applications', 2),\n",
       " ('Approach', 2),\n",
       " ('Conclusions and Future Work', 2),\n",
       " ('Summary', 2),\n",
       " ('English', 2),\n",
       " ('Resources', 2),\n",
       " ('Discussion and related work', 1),\n",
       " ('(-. &amp;quot;NN&amp;quot;)', 1),\n",
       " ('TRANSLATION DELIVERY SYSTEM', 1),\n",
       " ('TESTING AND EVALUATING MULTI-ENGINE PERFORMANCE', 1),\n",
       " ('CURRENT AND FUTURE WORK', 1),\n",
       " ('.. The Amount of Training Data Required', 1),\n",
       " ('Proper Name Identification in Natural Language Processing', 1),\n",
       " ('The Ambiguity of Proper Names', 1),\n",
       " ('Disambiguation Resources', 1),\n",
       " ('The Name Discovery Process', 1),\n",
       " ('Resolution of Structural Ambiguity', 1),\n",
       " ('Resolution of Ambiguity at', 1),\n",
       " ('Resolution of Semantic Ambiguity', 1),\n",
       " ('Description of the System', 1),\n",
       " ('Experimental Evaluation', 1),\n",
       " ('Conclusions and Further Work', 1),\n",
       " ('The K-vec Algorithm.', 1),\n",
       " ('General outline of the algorithm.', 1),\n",
       " ('Example output.', 1),\n",
       " ('Evaluation.', 1),\n",
       " ('Conc lus ion.', 1),\n",
       " ('Quasi-Synchronous Grammar.', 1),\n",
       " ('The Jeopardy Model.', 1),\n",
       " ('Experiments.', 1),\n",
       " ('Discussion.', 1),\n",
       " ('Conclusion.', 1),\n",
       " ('Discriminative Alignment Models.', 1),\n",
       " ('Alignment Search.', 1),\n",
       " ('Parameter Optimization.', 1),\n",
       " ('Data and Methodology for Evaluation.', 1),\n",
       " ('Experimental Results.', 1),\n",
       " ('Conclusions.', 1),\n",
       " ('Results (French).', 1),\n",
       " ('ARGMAX', 1),\n",
       " ('Appendix A', 1),\n",
       " ('DSO National Laboratories', 1),\n",
       " ('BOUGHT-BY: PHILIPS ({a, b, e}), SONY ({a, c, d,f}) COLOR: BROWN ({a,b}), YELLOW ({c,d})',\n",
       "  1),\n",
       " ('. Related Work', 1),\n",
       " ('Appendix: Complete List of Parameter Classes', 1),\n",
       " ('Appendix.', 1),\n",
       " ('end Acknowledgments', 1),\n",
       " ('. Model Estimation', 1),\n",
       " ('. Discussion', 1),\n",
       " ('. Methods for Incorporating Joint Information', 1),\n",
       " ('FORMAL CONSTITUTIVE', 1),\n",
       " ('AGENTIVE TELIC', 1),\n",
       " ('Cambridge, MA', 1),\n",
       " ('WORD ASSOCIATION NORMS, MUTUAL INFORMATION, AND LEXICOGRAPHY', 1),\n",
       " ('MEANING AND ASSOCIATION', 1),\n",
       " ('PRACTICAL APPLICATIONS', 1),\n",
       " ('WORD ASSOCIATION AND PSYCHOLINGUISTICS', 1),\n",
       " ('AN INFORMATION THEORETIC MEASURE', 1),\n",
       " ('LEXICO-SYNTACTIC REGULARITIES', 1),\n",
       " ('PREPROCESSING WITH A PART OF SPEECH TAGGER', 1),\n",
       " ('PREPROCESSING WITH A PARSER', 1),\n",
       " ('APPLICATIONS IN LEXICOGRAPHY', 1),\n",
       " ('CONCLUSIONS', 1),\n",
       " ('. met* Method', 1),\n",
       " ('Artificial Intelligence Program GE Research and Development Center', 1),\n",
       " ('Example', 1),\n",
       " ('Appendix: Program', 1),\n",
       " ('(lb)', 1),\n",
       " ('. Results Are Dependent on the Size of the Corpus', 1),\n",
       " ('. Results Are Dependent on the Contents of the Corpus', 1),\n",
       " ('Appendix A: Test Words', 1),\n",
       " ('Appendix B: Complete Output', 1),\n",
       " ('Appendix B: Summary of Models', 1),\n",
       " ('General Formulae.', 1),\n",
       " ('Keywords Count Comments', 1),\n",
       " ('. Intersentential and Intrasentential Anaphora in the Same Sentence', 1),\n",
       " ('AAAAAA', 1),\n",
       " ('Renaissance Technologies', 1),\n",
       " ('Acknowledgments Initiative in Cognitive Science and', 1),\n",
       " ('COMMAND STATEMENT QUESTION', 1),\n",
       " ('COMMUNICATION INFORMATION ACKNOWLEDGEMENT Does the response contain just',\n",
       "  1),\n",
       " ('SEGMENT X Meanwhile,', 1),\n",
       " ('AT&amp;T Laboratories', 1),\n",
       " ('o(x)= Hex)', 1),\n",
       " ('The Metropolis-Hastings acceptance probability is usually given in the form',\n",
       "  1),\n",
       " ('Appendix A: Initial Weight Estimation', 1),\n",
       " ('Appendix B: Adjusting Field Weights', 1),\n",
       " ('Appendix A: Instructions to the Annotators (First Experiment)', 1),\n",
       " ('Appendix B: Instructions to the Subjects (Second Experiment)', 1),\n",
       " ('SCRIPT', 1),\n",
       " ('Appendix A: Proof of Proposition', 1),\n",
       " ('Appendix A: Efficient On-Line Computation of', 1),\n",
       " ('Appendix B: Estimation of Boundary Statistics', 1),\n",
       " ('Appendix C: Speed vs.', 1),\n",
       " ('Definition: M-OPTIMIZE', 1),\n",
       " ('Path Differences', 1),\n",
       " ('Data Resources', 1),\n",
       " ('Synchronous Parsers', 1),\n",
       " ('Inference of Discontinuous Constituents', 1),\n",
       " ('Basic Idea', 1),\n",
       " ('Implementation', 1),\n",
       " ('Experiment and Evaluation', 1),\n",
       " ('Task Definition', 1),\n",
       " ('Lemma Alignment by Frequency Similarity', 1),\n",
       " ('Lemma Alignment by Morphological Transformation Probabilities', 1),\n",
       " ('Empirical Evaluation', 1),\n",
       " ('Paragraph Indexing', 1),\n",
       " ('Answer Processing', 1),\n",
       " ('Performance evaluation', 1),\n",
       " ('Previous Work', 1),\n",
       " ('The Immediate-Head Parsing Model', 1),\n",
       " ('Background and Motivation', 1),\n",
       " ('Transducers and Parameters', 1),\n",
       " ('Estimation in Parameterized FSTs', 1),\n",
       " ('The E Step: Expectation Semirings', 1),\n",
       " ('Removing Inefficiencies', 1),\n",
       " ('Processing Resources', 1),\n",
       " ('Language Resource Creation', 1),\n",
       " ('Problem Setting and Notation', 1),\n",
       " ('View Independence', 1),\n",
       " ('Rule Independence and Bootstrapping', 1),\n",
       " ('The Unreasonableness of Rule Independence', 1),\n",
       " ('Relaxing the Assumption', 1),\n",
       " ('The Greedy Agreement Algorithm', 1),\n",
       " ('The Yarowsky Algorithm', 1),\n",
       " ('The Direct Correspondence Assumption', 1),\n",
       " ('Direct Correspondence Assumption', 1),\n",
       " ('Evaluating the DCA using Annotation Projection', 1),\n",
       " ('Morpheme Segmentation', 1),\n",
       " ('Unsupervised Acquisition of New Stems', 1),\n",
       " ('Performance Evaluations', 1),\n",
       " ('Summary and Future Work', 1),\n",
       " ('Experimental Setup', 1),\n",
       " ('External vs. Internal Annotation', 1),\n",
       " ('Tag Splitting', 1),\n",
       " ('What is an Unlexicalized Grammar?', 1),\n",
       " ('Annotations Already in the Treebank', 1),\n",
       " ('Head Annotation', 1),\n",
       " ('Final Results', 1),\n",
       " ('..', 1),\n",
       " ('MSR-MT', 1),\n",
       " ('Characteristics of English SMS', 1),\n",
       " ('SMS Normalization', 1),\n",
       " ('. Phrase-based Model', 1),\n",
       " ('BLEU', 1),\n",
       " ('Direct Minimization of Error', 1),\n",
       " ('Training Log-Linear Models', 1),\n",
       " ('Deterministic Annealing', 1),\n",
       " ('Regularization', 1),\n",
       " ('Computing Expected Loss', 1),\n",
       " ('Toolkit', 1),\n",
       " ('Factored Translation Model', 1),\n",
       " ('Confusion Network Decoding', 1),\n",
       " ('. agreement', 1),\n",
       " ('Mary = [hair: blond]', 1),\n",
       " ('[', 1),\n",
       " ('speaker:', 1),\n",
       " ('I Overview', 1),\n",
       " ('II The Formalism', 1),\n",
       " ('III Translation', 1),\n",
       " ('...', 1),\n",
       " ('The Logical Notation', 1),\n",
       " ('Opaque Adverbials', 1),\n",
       " ('De Re and De Dicto Belief Reports', 1),\n",
       " ('Identity in Belief Contexts', 1),\n",
       " ('Ezist(Qt) A Exiat(Q)', 1),\n",
       " ('y)Evening-Star(z) A Evening-Star(y) rel-identieal(z, y)', 1),\n",
       " ('The Role of Semantics', 1),\n",
       " ('Acknowledgement.', 1),\n",
       " ('FocusList: Heventl],[drivel],[/-]]', 1),\n",
       " ('REFERENCES', 1),\n",
       " ('The Problem of Coherence', 1),\n",
       " ('Text Structuring', 1),\n",
       " ('Previous Approaches', 1),\n",
       " ('Formalizing RST Relations', 1),\n",
       " ('An Example', 1),\n",
       " ('Shortcomings and Further Work', 1),\n",
       " ('save X from Y ( concordance lines)  save PERSON from Y ( concordance lines)',\n",
       "  1),\n",
       " ('. save INST(ITUTION) from (ECON) BAD ( concordance lines)', 1),\n",
       " ('. save ANIMAL from DESTRUCT(ION) ( concordance lines)', 1),\n",
       " ('I.', 1),\n",
       " ('ACKNOWLEGMENTS', 1),\n",
       " ('Grammar and Model Performance Metrics', 1),\n",
       " ('The HBG Model', 1),\n",
       " ('Attribute grammar interpretation', 1),\n",
       " ('Actor-based GB parsing', 1),\n",
       " ('Method', 1),\n",
       " ('ITG and BTG Overview', 1),\n",
       " ('VP [VP PP] VP (VP PP)', 1),\n",
       " ('BTG-Based Search for the Original Models', 1),\n",
       " ('Replacing the Channel Model with a SBTG', 1),\n",
       " ('Charts', 1),\n",
       " ('Generation', 1),\n",
       " ('The Algorithm Schema', 1),\n",
       " ('Internal and External Indices', 1),\n",
       " ('Indexing', 1),\n",
       " ('Initial Observations', 1),\n",
       " ('Event Profile: WordNet and EVCA', 1),\n",
       " ('Results and Future Work.', 1),\n",
       " ('A Phrase-Based Joint Probability Model', 1),\n",
       " ('Training', 1),\n",
       " ('Decoding', 1),\n",
       " ('Word Graphs', 1),\n",
       " ('Pruning and Rescoring', 1),\n",
       " ('Defining a Word Alignment Shared Task', 1),\n",
       " ('Evaluation Measures', 1),\n",
       " ('Participating Systems', 1),\n",
       " ('Results and Discussion', 1),\n",
       " ('Abstract  Segmentation as Tagging', 1),\n",
       " ('Tagging Algorithms', 1),\n",
       " ('Systems and Scores', 1),\n",
       " ('Problem description', 1),\n",
       " ('Inter-annotator Agreement', 1),\n",
       " ('.. Error Analysis', 1),\n",
       " ('SRL System Architecture', 1),\n",
       " ('Inference with Multiple SRL Systems', 1),\n",
       " ('Learning and Evaluation', 1),\n",
       " ('METEOR: Metric for Evaluation of Translation with Explicit ORdering', 1),\n",
       " ('The METEOR Metric . Weaknesses in BLEU Addressed in METEOR', 1),\n",
       " ('Evaluation of the METEOR Metric', 1),\n",
       " ('Future Work', 1),\n",
       " ('Evaluation Framework', 1),\n",
       " ('Automatic Evaluation', 1),\n",
       " ('Manual Evaluation', 1),\n",
       " ('Adequacy Fluency  All Meaning Flawless English  Most Meaning Good English  Much Meaning Non-native English  Little Meaning Disfluent English  None Incomprehensible',\n",
       "  1),\n",
       " ('Results and Analysis', 1),\n",
       " ('Our framework: the Moses MT system', 1),\n",
       " ('Domain adaption', 1),\n",
       " ('Domain adaptation results', 1),\n",
       " ('WMT  shared task submissions', 1),\n",
       " ('The RTE- Dataset', 1),\n",
       " ('The RTE- Challenge', 1),\n",
       " ('SEMANTIC HIERARCHY', 1),\n",
       " ('SEMANTIC DISTANCE', 1),\n",
       " ('DI D', 1),\n",
       " ('TRAINING AND TESTING DATA', 1),\n",
       " ('AutoSlog-TS: generating simple extraction patterns', 1),\n",
       " ('Generating conceptual case frames from extraction patterns', 1),\n",
       " ('Examples', 1),\n",
       " ('Related work', 1),\n",
       " ('Word sense disambiguation of gloss concepts', 1),\n",
       " ('Logical form transformation', 1),\n",
       " ('Semantic form transformation', 1),\n",
       " ('Include more derivational morphology', 1),\n",
       " ('Statistical Machine Translation', 1),\n",
       " ('Single-Word Based Approach', 1),\n",
       " ('Alignment Template Approach', 1),\n",
       " ('Translation results', 1),\n",
       " ('Research Goals', 1),\n",
       " ('Definition of Similarity', 1),\n",
       " ('Methodology', 1),\n",
       " ('Analysis and Discussion of Feature Performance', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contador_sessoes(papers_sem_introducao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd8bad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_conclusion1(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        #txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        lista.append(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "778f7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_conclusion4(corpus: str):\n",
    "    regIter1 = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos1  = [ t.group(2) for t in regIter1]\n",
    "    regIter2 = re.finditer('(<SECTION.*Acknowledgments.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos2  = [ t.group(2) for t in regIter2]\n",
    "    regIter3 = re.finditer('(<SECTION.*Acknowledgements.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos3 = [ t.group(2) for t in regIter3]\n",
    "    texto=textos1\n",
    "    for i in textos2:\n",
    "        if i not in texto:\n",
    "            texto.append(i)\n",
    "    for i in (textos3)&(textos2):\n",
    "        if i not in texto:\n",
    "             texto.append(i)\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        #txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        lista.append(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9087827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_conclusion2(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*Acknowledgments.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    \n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        #txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        lista.append(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7de2054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_conclusion4(corpus: str):\n",
    "    regIter1 = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos1  = [ t.group(2) for t in regIter]\n",
    "    regIter2 = re.finditer('(<SECTION.*Acknowledgments.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos2  = [ t.group(2) for t in regIter]\n",
    "    regIter3 = re.finditer('(<SECTION.*Acknowledgements.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos3 = [ t.group(2) for t in regIter]\n",
    "    textos=textos1\n",
    "    for i in textos2:\n",
    "        if i not in textos:\n",
    "            textos.append(i)\n",
    "    for i in (textos3)&(textos2):\n",
    "        if i not in textos:\n",
    "             textos.append(i)\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        #txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        lista.append(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d71cff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_conclusion4(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    textos=textos1\n",
    "    return textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a66721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_conclusion4(corpus: str):\n",
    "    regIter1 = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos1  = [ t.group(2) for t in regIter1]\n",
    "    regIter2 = re.finditer('(<SECTION.*Acknowledgments.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos2  = [ t.group(2) for t in regIter2]\n",
    "    regIter3 = re.finditer('(<SECTION.*Acknowledgements.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos3 = [ t.group(2) for t in regIter3]\n",
    "    texto=textos1\n",
    "    for i in textos2:\n",
    "        if i not in texto:\n",
    "            texto.append(i)\n",
    "    for i in (textos3)&(textos2):\n",
    "        if i not in texto:\n",
    "             texto.append(i)\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        #txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        lista.append(txt)\n",
    "    return textos1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "76e89b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_conclusion4(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    return textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bf2ba29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_conclusion(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        #txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        lista.append(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b36defab",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste  = get_indices_preprocessamento(papers_com_indice, preprocessa_conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9b795e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_conclusion4(corpus: str):\n",
    "    regIter1 = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos1  = [ t.group(2) for t in regIter1]\n",
    "    #regIter2 = re.finditer('(<SECTION.*Acknowledgments.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    #textos2  = [ t.group(2) for t in regIter2]\n",
    "    #regIter3 = re.finditer('(<SECTION.*Acknowledgements.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    #textos3 = [ t.group(2) for t in regIter3]\n",
    "    #texto=textos1\n",
    "    for i in textos2:\n",
    "        if i not in texto:\n",
    "            texto.append(i)\n",
    "    for i in (textos3)&(textos2):\n",
    "        if i not in texto:\n",
    "             texto.append(i)\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        #txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        lista.append(txt)\n",
    "    return textos1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "76054503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_values_from_list(the_list, val):\n",
    "    return [value for value in the_list if value != val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4bf74a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_conclusion4(corpus: str):\n",
    "    regIter1 = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos1 = [ t.group(2) for t in regIter1]\n",
    "    regIter2 = re.finditer('(<SECTION.*Acknowledgments.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos2 = [ t.group(2) for t in regIter2]\n",
    "    regIter3 = re.finditer('(<SECTION.*Acknowledgements.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos3 = [ t.group(2) for t in regIter3]\n",
    "    return textos3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "eb388e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_conclusion4(corpus: str):\n",
    "    regIter1 = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos1 = [ t.group(2) for t in regIter1]\n",
    "    regIter2 = re.finditer('(<SECTION.*Acknowledgments.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos2 = [ t.group(2) for t in regIter2]\n",
    "    regIter3 = re.finditer('(<SECTION.*Acknowledgements.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos3 = [ t.group(2) for t in regIter3]\n",
    "    textos = textos1\n",
    "    for i in textos2:\n",
    "        if i not in textos:\n",
    "            textos.append(i)\n",
    "    for i in textos3:\n",
    "        if i not in textos:\n",
    "            textos.append(i)\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        #txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "de0b749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "for i in lista_papers:\n",
    "    try:\n",
    "        x.append(preprocessa_conclusion4(i))\n",
    "    except:\n",
    "        None\n",
    "x=remove_values_from_list(x,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ed2b3925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "827"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07f87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c47d4ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2123"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessa_conclusion(lista_papers[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f9a574b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "711"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ac26ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 22,\n",
       " 24,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 35,\n",
       " 37,\n",
       " 39,\n",
       " 41,\n",
       " 43,\n",
       " 45,\n",
       " 47,\n",
       " 48,\n",
       " 50,\n",
       " 51,\n",
       " 53,\n",
       " 54,\n",
       " 56,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 74,\n",
       " 90,\n",
       " 97,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 106,\n",
       " 108,\n",
       " 110,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 134,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 166,\n",
       " 167,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 192,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 222,\n",
       " 225,\n",
       " 226,\n",
       " 231,\n",
       " 233,\n",
       " 234,\n",
       " 237,\n",
       " 238,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 247,\n",
       " 248,\n",
       " 250,\n",
       " 253,\n",
       " 255,\n",
       " 256,\n",
       " 258,\n",
       " 260,\n",
       " 265,\n",
       " 267,\n",
       " 271,\n",
       " 272,\n",
       " 275,\n",
       " 276,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 289,\n",
       " 290,\n",
       " 296,\n",
       " 299,\n",
       " 302,\n",
       " 305,\n",
       " 307,\n",
       " 308,\n",
       " 315,\n",
       " 318,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 327,\n",
       " 330,\n",
       " 331,\n",
       " 333,\n",
       " 337,\n",
       " 339,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 369,\n",
       " 370,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 388,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 427,\n",
       " 428,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 451,\n",
       " 452,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 493,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 520,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 529,\n",
       " 530,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 559,\n",
       " 560,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 588,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 663,\n",
       " 664,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 684,\n",
       " 685,\n",
       " 687,\n",
       " 688,\n",
       " 695,\n",
       " 699,\n",
       " 704,\n",
       " 706,\n",
       " 711,\n",
       " 712,\n",
       " 714,\n",
       " 716,\n",
       " 719,\n",
       " 720,\n",
       " 723,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 732,\n",
       " 733,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 740,\n",
       " 741,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 757,\n",
       " 759,\n",
       " 760,\n",
       " 762,\n",
       " 763,\n",
       " 765,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 776,\n",
       " 777,\n",
       " 779,\n",
       " 780,\n",
       " 783,\n",
       " 785,\n",
       " 786,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 809,\n",
       " 811,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 821,\n",
       " 822,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 830,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 865,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 896,\n",
       " 897,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 924,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 945,\n",
       " 946,\n",
       " 949,\n",
       " 950,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 981,\n",
       " 982,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 1002,\n",
       " 1003,\n",
       " 1004,\n",
       " 1005,\n",
       " 1006,\n",
       " 1007,\n",
       " 1008]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c69530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_conclusion3(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*Acknowledgements.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        #txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        lista.append(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_papers_com_conclusao   = get_indices_preprocessamento(papers_com_indice, preprocessa_conclusion)\n",
    "indices_papers_com_conclusao2  = get_indices_preprocessamento(papers_com_indice, preprocessa_conclusion2)\n",
    "indices_papers_com_conclusao3  = get_indices_preprocessamento(papers_com_indice, preprocessa_conclusion3)\n",
    "\n",
    "x=[]\n",
    "for i in indices_papers_com_conclusao2:\n",
    "    if i not in (indices_papers_com_conclusao+indices_papers_com_conclusao3):\n",
    "        x.append(i)\n",
    "indices_papers_com_conclusao2 = x \n",
    "\n",
    "x=[]\n",
    "for i in indices_papers_com_conclusao3:\n",
    "    if i not in indices_papers_com_conclusao:\n",
    "        x.append(i)\n",
    "indices_papers_com_conclusao3 = x \n",
    "\n",
    "conclusao1 =  dict((key,value) for key, value in papers_com_indice.items() if key in  indices_papers_com_conclusao)\n",
    "conclusao2 =  dict((key,value) for key, value in papers_com_indice.items() if key in  indices_papers_com_conclusao2)\n",
    "conclusao3 = dict((key,value) for key, value in papers_com_indice.items() if key in  indices_papers_com_conclusao3)\n",
    "\n",
    "conclusao = dict(conclusao1)\n",
    "conclusao.update(conclusao2)\n",
    "conclusao.update(conclusao3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8882ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(d4.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abdfdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results, Discussion, Concluding remarks, Experiments and Results, Conclustion and Future Research\n",
    "contador_sessoes(papers_sem_conclusao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd2132f",
   "metadata": {},
   "source": [
    "Identificação dos papers com as 3 sessões simultâneamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_papers=[]\n",
    "for idx in list(indices_papers_com_abstract):\n",
    "    if idx in list(indices_papers_com_introducao):\n",
    "        if idx in list(indices_papers_com_conclusao):\n",
    "            indice_papers.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Quantidade de papers com as três sessões simulataneamente:',len(indice_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbcf7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_faltantes=[]\n",
    "for i in indices:\n",
    "    if i not in indice_papers:\n",
    "        indices_faltantes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fae26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Quantidade de papers sem as três sessões:',len(indices_faltantes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba5e874",
   "metadata": {},
   "source": [
    "papers_de_fora = dict((key,value) for key, value in papers_com_indice.items() if key  in indices_faltantes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a47420",
   "metadata": {},
   "source": [
    "contador_sessoes(papers_de_fora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da47086",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = dict((key,value) for key, value in papers_com_indice.items() if key in indice_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de5e0f",
   "metadata": {},
   "source": [
    "Seguir o estudo com os artigos armazenados na variável `papers`. \\\n",
    "Os armazenados na variável `papers_de_fora` não entrarão no estudo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3bb482",
   "metadata": {},
   "source": [
    "# Separação das Seções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d708ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definição do Abstract\n",
    "abstract=[]\n",
    "for i in papers.values():\n",
    "    abstract.append(preprocessa_abstract(i))\n",
    "    \n",
    "#Definição da Introdução\n",
    "intro=[]\n",
    "for i in papers.values():\n",
    "    intro.append(preprocessa_intro(i))\n",
    "    \n",
    "#Definição da Conclusão\n",
    "conclusion=[]\n",
    "for i in papers.values():\n",
    "    conclusion.append(preprocessa_conclusion(i))\n",
    "    \n",
    "#Tokens\n",
    "token_abstract   = []\n",
    "for texto in abstract:\n",
    "    token_abstract.append(texto.split())\n",
    "    \n",
    "token_intro   = []\n",
    "for texto in intro:\n",
    "    token_intro.append(texto.split())\n",
    "    \n",
    "token_conclusion   = []\n",
    "for texto in conclusion:\n",
    "    token_conclusion.append(texto.split())\n",
    "\n",
    "tokens = token_abstract+token_intro+token_conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c7840",
   "metadata": {},
   "source": [
    "# Experimento 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb641b5f",
   "metadata": {},
   "source": [
    "## Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e20be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('model'):\n",
    "    model = joblib.load('model')\n",
    "else:\n",
    "    model = Word2Vec(tokens, min_count=1, epochs=100)\n",
    "    joblib.dump(model,'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e13f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6071ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "string=re.sub(\"\\(.*?\\)\",\"\",string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebf175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar('no', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f85cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('contrary', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('statistical', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ba4d8",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b12e6",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/64261546/how-to-solve-error-microsoft-visual-c-14-0-or-greater-is-required-when-inst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07110ab",
   "metadata": {},
   "source": [
    "from glove import Corpus, Glove\n",
    "\n",
    "#Creating a corpus object\n",
    "corpus = Corpus() \n",
    "\n",
    "#Training the corpus to generate the co-occurrence matrix which is used in GloVe\n",
    "corpus.fit(lines, window=10)\n",
    "\n",
    "glove = Glove(no_components=5, learning_rate=0.05) \n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('glove.model')\n",
    "\n",
    "https://github.com/maciejkula/glove-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb0ee1",
   "metadata": {},
   "source": [
    "## Fast Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe706b6",
   "metadata": {},
   "source": [
    "import fasttext\n",
    "\n",
    "\n",
    "model = fasttext.train_unsupervised('data.txt', model='skipgram')\n",
    "\n",
    "\n",
    "model = fasttext.train_unsupervised('data.txt', model='cbow')\n",
    "\n",
    "https://pypi.org/project/fasttext/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9f12ca",
   "metadata": {},
   "source": [
    "## Medida de similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e2bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessa_intro(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*[Ii][nN][tT][rR][oO][dD][Uu][Cc][Tt][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        txt = txt.strip()\n",
    "        lista.append(txt)\n",
    "    return txt\n",
    "\n",
    "def preprocessa_abstract(corpus: str):\n",
    "    regIter = re.finditer(\"(<[Aa][Bb][sS][tT][Rr][aA][cC][tT]>)(.+?)(</[aA][bB][sS][tT][rR][aA][cC][tT]>)\", corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        txt = txt.strip()\n",
    "        lista.append(txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def preprocessa_conclusion(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        txt = txt.strip()\n",
    "        lista.append(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcao_frase_palavra(frases_abstract):\n",
    "    \"\"\"input: lista de frases\n",
    "        output: lista de palavras de cada frase\"\"\"\n",
    "    palavras_frases=[]\n",
    "    for i in frases_abstract:\n",
    "        palavras_frases.append(i.split(' '))\n",
    "    try:\n",
    "        palavras_frases.remove('')\n",
    "    except:\n",
    "        return palavras_frases\n",
    "    return palavras_frases\n",
    "\n",
    "def vetor_frases(palavras_frases):\n",
    "    \"\"\"input: lista de palavras de cada frase\n",
    "        output: lista de vetores de cada frase\"\"\"\n",
    "    x=[]\n",
    "    for i in palavras_frases:\n",
    "        soma=0\n",
    "        for j in i:\n",
    "            try:\n",
    "                soma = soma +model.wv[j]\n",
    "            except:\n",
    "                soma=soma+0\n",
    "        x.append(soma)\n",
    "    return x\n",
    "\n",
    "def funcao_sim_coseno(a,b):\n",
    "    if ((numpy.linalg.norm(a)==0) |(numpy.linalg.norm(b)==0)):\n",
    "        return 0\n",
    "    else:\n",
    "        valor_coseno = (numpy.dot(a, b))/(numpy.linalg.norm(a)* numpy.linalg.norm(b))\n",
    "        return valor_coseno\n",
    "    \n",
    "def dicionario_similaridade(vetor_a,vetor_b):\n",
    "    \"\"\"Input: vetor correspondente às frases das seções a e b\n",
    "    Output: \n",
    "    \"\"\"\n",
    "    dicionario={}\n",
    "    for i in range(len(vetor_a)):\n",
    "        for j in range(len(vetor_b)):\n",
    "            dicionario[i,j]= funcao_sim_coseno(vetor_a[i],vetor_b[j])\n",
    "    return dicionario\n",
    "\n",
    "def remove_values_from_list(the_list, val):\n",
    "    return [value for value in the_list if value != val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e214d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definição do Abstract\n",
    "abstract=[]\n",
    "for i in papers.values():\n",
    "    abstract.append(preprocessa_abstract(i))\n",
    "    \n",
    "#Definição da Introdução\n",
    "intro=[]\n",
    "for i in papers.values():\n",
    "    intro.append(preprocessa_intro(i))\n",
    "    \n",
    "#Definição da Conclusão\n",
    "conclusion=[]\n",
    "for i in papers.values():\n",
    "    conclusion.append(preprocessa_conclusion(i))\n",
    "    \n",
    "#Tokens\n",
    "token_abstract   = []\n",
    "for texto in abstract:\n",
    "    token_abstract.append(texto.split())\n",
    "    \n",
    "token_intro   = []\n",
    "for texto in intro:\n",
    "    token_intro.append(texto.split())\n",
    "    \n",
    "token_conclusion   = []\n",
    "for texto in conclusion:\n",
    "    token_conclusion.append(texto.split())\n",
    "\n",
    "tokens = token_abstract+token_intro+token_conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduction\n",
    "frases_intro=[]\n",
    "for i in intro:\n",
    "    frases_intro.append(remove_values_from_list(i.split('.'),''))\n",
    "    \n",
    "    \n",
    "frases_intro3=[]\n",
    "for i in frases_intro:\n",
    "    frases_intro2=[]\n",
    "    for j in i:\n",
    "        frases_intro2.append(j.strip())\n",
    "    frases_intro3.append(frases_intro2)\n",
    "    \n",
    "frases_intro = frases_intro3 \n",
    "\n",
    "\n",
    "palavras_intro=[]\n",
    "for i in frases_intro:\n",
    "    palavras_intro.append(funcao_frase_palavra(i))\n",
    "    \n",
    "vetor_intro=[]\n",
    "for i in palavras_intro:\n",
    "    vetor_intro.append(vetor_frases(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be22a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abstract\n",
    "frases_abstract=[]\n",
    "for i in abstract:\n",
    "    frases_abstract.append(remove_values_from_list(i.split('.'),''))\n",
    "    \n",
    "    \n",
    "frases_abstract3=[]\n",
    "for i in frases_abstract:\n",
    "    frases_abstract2=[]\n",
    "    for j in i:\n",
    "        frases_abstract2.append(j.strip())\n",
    "    frases_abstract3.append(frases_abstract2)\n",
    "    \n",
    "frases_abstract = frases_abstract3 \n",
    "\n",
    "\n",
    "palavras_abstract=[]\n",
    "for i in frases_abstract:\n",
    "    palavras_abstract.append(funcao_frase_palavra(i))\n",
    "    \n",
    "vetor_abstract=[]\n",
    "for i in palavras_abstract:\n",
    "    vetor_abstract.append(vetor_frases(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c692ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion\n",
    "frases_conclusion=[]\n",
    "for i in conclusion:\n",
    "    frases_conclusion.append(remove_values_from_list(i.split('.'),''))\n",
    "    \n",
    "    \n",
    "frases_conclusion3=[]\n",
    "for i in frases_conclusion:\n",
    "    frases_conclusion2=[]\n",
    "    for j in i:\n",
    "        frases_conclusion2.append(j.strip())\n",
    "    frases_conclusion3.append(frases_conclusion2)\n",
    "frases_conclusion = frases_conclusion3 \n",
    "\n",
    "palavras_conclusion=[]\n",
    "for i in frases_conclusion:\n",
    "    palavras_conclusion.append(funcao_frase_palavra(i))\n",
    "    \n",
    "vetor_conclusion=[]\n",
    "for i in palavras_conclusion:\n",
    "    vetor_conclusion.append(vetor_frases(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c2e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionario_similaridade(vetor_abstract[0], vetor_intro[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00100f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similaridade entre Abstract e Introdução\n",
    "x = []\n",
    "for i,j in zip(range(len(vetor_abstract)),range(len(vetor_intro))):\n",
    "    x.append(dicionario_similaridade(vetor_abstract[i], vetor_intro[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32317b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_indices_ab_intro = []\n",
    "for i in range(len(x)):\n",
    "    top_3_indices_ab_intro.append(sorted(x[i], key=x[i].get, reverse=True)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_indices_ab_intro[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in top_3_indices_ab_intro[0]:\n",
    "    print(x[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d43e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_1=[]\n",
    "for i in top_3_indices_ab_intro:\n",
    "    top3=[]\n",
    "    for j in i:\n",
    "        top3.append(j[0])\n",
    "    lista_1.append(list(set(top3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b2872",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similaridade entre Abstract e Conclusão\n",
    "x = []\n",
    "for i,j in zip(range(len(vetor_abstract)),range(len(vetor_conclusion))):\n",
    "    x.append(dicionario_similaridade(vetor_abstract[i], vetor_conclusion[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b52ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maiores indices\n",
    "top_3_indices_ab_conc = []\n",
    "for i in range(len(x)):\n",
    "    top_3_indices_ab_conc.append(sorted(x[i], key=x[i].get, reverse=True)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933ae5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_indices_ab_conc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150c9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in top_3_indices_ab_conc[0]:\n",
    "    print(x[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68db139",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_2=[]\n",
    "for i in top_3_indices_ab_conc:\n",
    "    top3=[]\n",
    "    for j in i:\n",
    "        top3.append(j[0])\n",
    "    lista_2.append(list(set(top3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b145cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270aabc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_2[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20d980",
   "metadata": {},
   "source": [
    "Intersecção entre as frases do abstract que são similares simultâneamente à introdução e a conclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af00b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [[1,2,3,4], [2,3,4], [3,4,5,6,7]]\n",
    "set.intersection(*map(set,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_objetivo=[]\n",
    "for i,j in zip(lista_1,lista_2):\n",
    "    indices_objetivo.append(list(set.intersection(*map(set,[i,j]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e52f01",
   "metadata": {},
   "source": [
    "#Pegando os índices em comum entre as seções abstract+intro e abstract+conc\n",
    "indices_objetivo=[]\n",
    "for i, j in zip(lista_1,lista_2):\n",
    "    idx_um_art=[]\n",
    "    for h, z in zip(i,j):\n",
    "        if h==z:\n",
    "            idx_um_art.append(h)\n",
    "    indices_objetivo.append(idx_um_art)\n",
    "lista_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2360d033",
   "metadata": {},
   "source": [
    "#Pegando os índices em comum entre as seções abstract+intro e abstract+conc\n",
    "indices_objetivo=[]\n",
    "for i, j in zip(lista_1,lista_2):\n",
    "    idx_um_art=[]\n",
    "    for h, z in zip(i,j):\n",
    "        if h==z:\n",
    "            idx_um_art.append(h)\n",
    "    indices_objetivo.append(idx_um_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e136059",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases_abstract[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_objetivo[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923aa01",
   "metadata": {},
   "source": [
    "`indices_objetivo` índices da frase objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases_abstract[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_objetivo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c54487",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases_abstract[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases_abstract[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_total=[]\n",
    "for idx_f,idx_ind in zip(range(len(frases_abstract[0:3])),indices_objetivo[0:3]):\n",
    "    lista_one=[]\n",
    "    for j in idx_ind:\n",
    "        lista_one.append(frases_abstract[idx_f][j])\n",
    "        #print(idx_f)\n",
    "        #print(j)\n",
    "        #print(frases_abstract[idx_f][j])\n",
    "    papers_total.append(lista_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49447994",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc43d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_objetivo=[]\n",
    "for idx_f,idx_ind in zip(range(len(frases_abstract)),indices_objetivo):\n",
    "    lista_one=[]\n",
    "    for j in idx_ind:\n",
    "        lista_one.append(frases_abstract[idx_f][j])\n",
    "        #print(idx_f)\n",
    "        #print(j)\n",
    "        #print(frases_abstract[idx_f][j])\n",
    "    frase_objetivo.append(lista_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29458d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314dc457",
   "metadata": {},
   "source": [
    "frase_objetivo = []\n",
    "for i in range(len(frases_abstract)):\n",
    "    if i in indices_objetivo:\n",
    "        frase_objetivo.append(frases_abstract[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90238b3f",
   "metadata": {},
   "source": [
    "frase_objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab7c6a",
   "metadata": {},
   "source": [
    "Saída para treinamento do modelo de linguagem:\n",
    "`frase_objetivo`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc494b0",
   "metadata": {},
   "source": [
    "# Leitura da Sumarização Padrão Ouro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6081303",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('lista_padrao_ouro'):\n",
    "    lista_padrao_ouro = joblib.load('lista_padrao_ouro')\n",
    "else:\n",
    "    list_of_files = os.listdir('scisummnet_release1.1__20190413/top1000_complete')\n",
    "\n",
    "    list_of_files2= []\n",
    "    for i in list_of_files:\n",
    "        nome = 'scisummnet_release1.1__20190413/top1000_complete/' + i + '/' + 'summary/' +os.listdir(f'scisummnet_release1.1__20190413/top1000_complete/{i}/summary')[0]\n",
    "        list_of_files2.append(nome)\n",
    "\n",
    "\n",
    "    lines = []\n",
    "    for file in list_of_files2:\n",
    "        f= open(file,'r', encoding='utf-8' )\n",
    "        lines.append(f.readlines())\n",
    "        f.close()\n",
    "\n",
    "    lista_padrao_ouro=[]\n",
    "    for j in lines:\n",
    "        texto=''\n",
    "        for i in j:\n",
    "            texto = texto + str(i)\n",
    "        lista_papers.append(texto)\n",
    "\n",
    "        joblib.dump(lista_padrao_ouro, 'lista_padrao_ouro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d71a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lista_padrao_ouro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720bdb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicando indices aos papers\n",
    "indices           = range(0,len(lista_papers))\n",
    "padrao_ouro = dict(zip(indices, lista_padrao_ouro))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179862f",
   "metadata": {},
   "source": [
    "Filtro dos sumários correspondentes aos papers que possuem as Seções Abstract, Introdução e Conclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7786b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "selecao_padrao_ouro = dict((key,value) for key, value in padrao_ouro.items() if key in  indice_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacd018",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selecao_padrao_ouro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6593de",
   "metadata": {},
   "source": [
    "# ROUGE SCORE\n",
    "Aplicação do `ROUGE` entre as frase identificadas como objetivo e as frases padrão ouro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643aa46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8f0ef",
   "metadata": {},
   "source": [
    "def pre_processamento_ouro(txt):\n",
    "    txt = re.sub(r'\\n.*','', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selecao_padrao_ouro = list(selecao_padrao_ouro.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38567ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_frases_padrao_ouro=[]\n",
    "for i in selecao_padrao_ouro:\n",
    "    lista_frases_padrao_ouro.append(i.split('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ab1fcd",
   "metadata": {},
   "source": [
    "for i,j in zip(frase_objetivo[0:2],lista_frases_padrao_ouro[0:2]):\n",
    "    for h in range(len(i)):\n",
    "        for z in range(len(j)):\n",
    "            print(scorer.score(i[h],j[z])['rouge1'].fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_total=[]\n",
    "for i,j in zip(frase_objetivo,lista_frases_padrao_ouro):\n",
    "    rouge_1={}\n",
    "    for h in range(len(i)):\n",
    "        for z in range(len(j)):\n",
    "            rouge_1[(h,z)] = (scorer.score(i[h],j[z])['rouge1'].fmeasure)\n",
    "    rouge_total.append(rouge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_frases_padrao_ouro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7bfb3",
   "metadata": {},
   "source": [
    "#conferindo\n",
    "rouge_1={}\n",
    "for i in range(len(frase_objetivo[0])):\n",
    "    for j in range(len(frases_padra_ouro)):\n",
    "        rouge_1[(i,j)] = (scorer.score(frase_objetivo[0][i],frases_padra_ouro[j])['rouge1'].fmeasure)\n",
    "rouge_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(dic, coord, val):\n",
    "    return max(filter(lambda item: item[0][coord] == val, dic.items()), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa34599",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximo_rouge_por_paper =[]\n",
    "for h in rouge_total:\n",
    "    x=[]\n",
    "    for i in set([i[0] for i in h]):\n",
    "        x.append(get_max(h,0,i))\n",
    "    maximo_rouge_por_paper.append(x)\n",
    "maximo_rouge_por_paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0dc1a",
   "metadata": {},
   "source": [
    "x=[]\n",
    "for i in maximo_rouge_por_paper:\n",
    "    soma=0\n",
    "    for j in i:\n",
    "        soma = soma + j[1]\n",
    "    #print(len(i))\n",
    "    try:\n",
    "        x.append(soma)\n",
    "    except:\n",
    "        x.append(0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisao=[]\n",
    "for i in maximo_rouge_por_paper:\n",
    "    soma=0\n",
    "    for j in i:\n",
    "        soma = soma + j[1]\n",
    "    #print(len(i))\n",
    "    try:\n",
    "        precisao.append(soma/len(i))\n",
    "    except:\n",
    "        precisao.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d035813",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lista_frases_padrao_ouro:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b5ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ae55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "revocacao=[]\n",
    "for i,h in zip(maximo_rouge_por_paper,lista_frases_padrao_ouro):\n",
    "    soma=0\n",
    "    for j in i:\n",
    "        soma = soma + j[1]\n",
    "    #print(len(i))\n",
    "    try:\n",
    "        revocacao.append(soma/len(h))\n",
    "    except:\n",
    "        revocacao.append(0)\n",
    "revocacao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a1997",
   "metadata": {},
   "source": [
    "frases_padra_ouro ta errado acima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca82f49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(lista_frases_padrao_ouro[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a209d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(precisao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93097308",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(Counter(precisao), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb25034",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(precisao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(revocacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938da9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(Counter(revocacao), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71309d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(precisao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d4881",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max(revocacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a49e662",
   "metadata": {},
   "source": [
    "tentar guardar a identação - precisão e recall de quais papers?\\\n",
    "testar a remoção das stop words do word2vec\\\n",
    "adicionar os termos similares na conclusão\\\n",
    "glove e fasttext\\\n",
    "treino do model language\\\n",
    "--------\n",
    "correções na quali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ff612",
   "metadata": {},
   "source": [
    "# Modelo de Linguagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269c445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
